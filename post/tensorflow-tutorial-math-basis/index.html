<!DOCTYPE html>
<html lang="zh-cn">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>Tensorflow 入门之数学基础 - Bigshans&#39; Blog</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="bigshans" /><meta name="description" content="数学是通往机器学习不可避免的路径，它即是阶梯也是拦路虎。今天我们就来学习一下 Tensorflow 的一些简单的数学基础，并且与 Tensorflow 的数据结构进行结合，使我们能够不" /><meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.89.0-DEV with theme even" />


<link rel="canonical" href="https://bigshans.github.io/post/tensorflow-tutorial-math-basis/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">



<link href="/sass/main.min.b0c6a65638aed399d04b625166fb172e2f3ddf4538702b72f6f65c05c7dc4db1.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">
<link rel="stylesheet" href="/css/custom.css">


<meta property="og:title" content="Tensorflow 入门之数学基础" />
<meta property="og:description" content="数学是通往机器学习不可避免的路径，它即是阶梯也是拦路虎。今天我们就来学习一下 Tensorflow 的一些简单的数学基础，并且与 Tensorflow 的数据结构进行结合，使我们能够不" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bigshans.github.io/post/tensorflow-tutorial-math-basis/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2021-10-18T16:06:11+08:00" />
<meta property="article:modified_time" content="2021-10-18T16:06:11+08:00" />

<meta itemprop="name" content="Tensorflow 入门之数学基础">
<meta itemprop="description" content="数学是通往机器学习不可避免的路径，它即是阶梯也是拦路虎。今天我们就来学习一下 Tensorflow 的一些简单的数学基础，并且与 Tensorflow 的数据结构进行结合，使我们能够不"><meta itemprop="datePublished" content="2021-10-18T16:06:11+08:00" />
<meta itemprop="dateModified" content="2021-10-18T16:06:11+08:00" />
<meta itemprop="wordCount" content="2723">
<meta itemprop="keywords" content="machine learing,tensorflow," /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Tensorflow 入门之数学基础"/>
<meta name="twitter:description" content="数学是通往机器学习不可避免的路径，它即是阶梯也是拦路虎。今天我们就来学习一下 Tensorflow 的一些简单的数学基础，并且与 Tensorflow 的数据结构进行结合，使我们能够不"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">Bigshans</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item fa">
          
            <i class="fa-home"></i>
          

          首页
        </li>
      </a><a href="/post/">
        <li class="mobile-menu-item fa">
          
            <i class="fa-archives"></i>
          

          归档
        </li>
      </a><a href="/tags/">
        <li class="mobile-menu-item fa">
          
            <i class="fa-tags"></i>
          

          标签
        </li>
      </a><a href="/categories/">
        <li class="mobile-menu-item fa">
          
            <i class="fa-categories"></i>
          

          分类
        </li>
      </a>
  </ul>

  


</nav>

  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">Bigshans</a>
</div>





<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <span class="fa">
          
            <i class="fa-home"></i>
          
        </span>
        <a class="menu-item-link" href="/">首页</a>
      </li><li class="menu-item">
        <span class="fa">
          
            <i class="fa-archives"></i>
          
        </span>
        <a class="menu-item-link" href="/post/">归档</a>
      </li><li class="menu-item">
        <span class="fa">
          
            <i class="fa-tags"></i>
          
        </span>
        <a class="menu-item-link" href="/tags/">标签</a>
      </li><li class="menu-item">
        <span class="fa">
          
            <i class="fa-categories"></i>
          
        </span>
        <a class="menu-item-link" href="/categories/">分类</a>
      </li>
  </ul>
</nav>

    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">Tensorflow 入门之数学基础</h1>

      <div class="post-meta">
        <span class="post-time"> 2021-10-18 </span>
        <div class="post-category">
            <a href="/categories/tensorflow/"> tensorflow </a>
            </div>
          <span class="more-meta"> 约 2723 字 </span>
          <span class="more-meta"> 预计阅读 6 分钟 </span>
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">文章目录</h2>
  <div class="post-toc-content"><nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#导入">导入</a></li>
        <li><a href="#线性代数">线性代数</a>
          <ul>
            <li><a href="#标量">标量</a></li>
            <li><a href="#向量">向量</a></li>
            <li><a href="#矩阵">矩阵</a></li>
            <li><a href="#张量">张量</a></li>
            <li><a href="#张量算法的基本性质">张量算法的基本性质</a></li>
            <li><a href="#降维">降维</a></li>
            <li><a href="#点积">点积</a></li>
            <li><a href="#矩阵向量积">矩阵向量积</a></li>
            <li><a href="#矩阵乘法">矩阵乘法</a></li>
            <li><a href="#范数">范数</a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav></div>
</div>

    <div class="post-content">
      

<p>数学是通往机器学习不可避免的路径，它即是阶梯也是拦路虎。今天我们就来学习一下 Tensorflow 的一些简单的数学基础，并且与 Tensorflow 的数据结构进行结合，使我们能够不失抽象的情况下直观地与代码想联系。不多说，开始吧！</p>
<h2 id="导入">导入</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span></code></pre></div>
<h2 id="线性代数">线性代数</h2>
<h3 id="标量">标量</h3>
<p>所谓标量，就是只有大小、没有方向、可用实数表示的一个量。在 tensorflow 中，我们使用 只有一个元素的张量表示，它可以被正常用于加减乘除。对于我们来说，实际上它就是一个数。</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> tf.constant([<span class="fl">3.0</span>])</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> tf.constant([<span class="fl">2.0</span>])</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">+</span> y, x <span class="op">*</span> y, x <span class="op">/</span> y, x<span class="op">**</span>y</span></code></pre></div>
<h3 id="向量">向量</h3>
<p>在原来标量的维度上多了方向这一维度，就成了向量。你可以将向量视为标量组成的列表，在几何上，它往往代表了坐标系上的一个点，其方向是零点指向对应点。</p>
<p>在 tensorflow 里我们使用一维张量处理向量。</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> tf.<span class="bu">range</span>(<span class="dv">4</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>x</span></code></pre></div>
<p>习惯上，我们使用列向量来表示： <span class="math display">\[
\mathbf{x} = \left[ \\
\begin{matrix}
a_1 \\
a_2 \\
\vdots \\
a_n \\
\end{matrix}
\right], \tag{1, 1}
\]</span></p>
<p>其中 <span class="math inline">\(a_1 , \dots, a_n\)</span> 都是向量的元素，我们使用索引来访问任何一个元素。</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>x[<span class="dv">3</span>]</span></code></pre></div>
<p>显而易见的，向量只有一维，它因此存在一个长度。通过 <code>len()</code> 即可以获得。</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(x)</span></code></pre></div>
<p>因为我们实际上是把它当作一个特殊的张量来使用的，它实际上还存在一个 <code>shape</code> ，只不过是一维的。</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>x.shape</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="co"># TensorShape([4])</span></span></code></pre></div>
<h3 id="矩阵">矩阵</h3>
<p>我们一步步前进你就会发现，标量到向量，意味着我们从零维到达了一维，而从向量到矩阵，我们则从一维到达了二维。矩阵在形式上是由 <span class="math inline">\(m \times n\)</span> 个元素组成的 <span class="math inline">\(m\)</span> 行 <span class="math inline">\(n\)</span> 列的元素几何，在代码中，被表示为具有两个轴的张量。</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> tf.reshapre(tf.<span class="bu">range</span>(<span class="dv">20</span>), (<span class="dv">5</span>, <span class="dv">4</span>))</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>A</span></code></pre></div>
<p>这里我们使用 <code>reshapre()</code> 创建了一个矩阵。</p>
<p>在数学表示法中，我们使用 <span class="math inline">\(\mathbf{A} \in \mathbb{R}^{m\times n}\)</span> 来表示矩阵，其由 <span class="math inline">\(m\)</span> 行和 <span class="math inline">\(n\)</span> 列的实值标量组成。直观地，我们可以将任意矩阵 <span class="math inline">\(\mathbf{A} \in \mathbb{R}^{m\times n}\)</span> 视为一个表格，其中每个元素 <span class="math inline">\(a_{ij}\)</span> 属于第 <span class="math inline">\(i\)</span> 行第 <span class="math inline">\(j\)</span> 列： <span class="math display">\[
\mathbf{x} = \left[
\begin{matrix}
a_{11} &amp; a_{12} &amp; \dots &amp; a_{1n} \\
a_{21} &amp; a_{22} &amp; \dots &amp; a_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
a_{m1} &amp; a_{m2} &amp; \dots &amp; a_{mn} \\
\end{matrix}
\right], \tag{2, 1}
\]</span> 如果 <span class="math inline">\(m = n\)</span> ，则称之为方矩阵。</p>
<p>有时候我们想要翻转矩阵。当我们交换举轴和列时，其结果成为矩阵的转置（transpose）。我们用 <span class="math inline">\(\mathbf{B} = \mathbf{A^\mathrm{T}}\)</span> 。我用使用 <code>tf.transpose()</code> 进行转置。</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>tf.transpose(A)</span></code></pre></div>
<p>比如我们对 <code>(2, 1)</code> 进行转置就得到了 <code>(2, 2)</code> 。 <span class="math display">\[
\mathbf{x} = \left[
\begin{matrix}
a_{11} &amp; a_{21} &amp; \dots &amp; a_{m1} \\
a_{12} &amp; a_{22} &amp; \dots &amp; a_{m2} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
a_{1n} &amp; a_{n2} &amp; \dots &amp; a_{mn} \\
\end{matrix}
\right], \tag{2, 2}
\]</span> 如果 <span class="math inline">\(\mathbf{A} = \mathbf{A^{\mathrm{T}}}\)</span> ，则该矩阵我们称为对称矩阵。</p>
<p>我们可以用代码简单证明一下。</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> tf.constant([[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], [<span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">4</span>], [<span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>]])</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>B <span class="op">==</span> tftranspose(B)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co">&#39;&#39;&#39;</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="co">output:</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;tf.Tensor: shape=(3, 3), dtype=bool, numpy=</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="co">array([[ True,  True,  True],</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co">       [ True,  True,  True],</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="co">       [ True,  True,  True]])&gt;</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="co">&#39;&#39;&#39;</span></span></code></pre></div>
<h3 id="张量">张量</h3>
<p>我们使用多维数组表示张量，即是用分量的多维数组来表示。其实在之前的内容中，我们一直在使用张量来表示标量、向量、矩阵，这使得我们这样理解张量：张量是以上数据结构的更高维度。从计算机的角度看并没有问题，因为它确实是这样表示的，但在数学上，张量会更复杂一些，不过我们不纠结。</p>
<p>想看的话戳这里：https://www.bilibili.com/video/BV1dJ411W7Xm/ 。</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> tf.reshapre(tf.<span class="bu">range</span>(<span class="dv">24</span>), (<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>))</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>X</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co">&#39;&#39;&#39;</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co">output:</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;tf.Tensor: shape=(2, 3, 4), dtype=int32, numpy=</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="co">array([[[ 0,  1,  2,  3],</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="co">        [ 4,  5,  6,  7],</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="co">        [ 8,  9, 10, 11]],</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="co">       [[12, 13, 14, 15],</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="co">        [16, 17, 18, 19],</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="co">        [20, 21, 22, 23]]], dtype=int32)&gt;</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a><span class="co">&#39;&#39;&#39;</span></span></code></pre></div>
<h3 id="张量算法的基本性质">张量算法的基本性质</h3>
<p>我们可以将任意两个具有相同形状的张量进行二元运算，得到的结果也必然是相同形状的张量。</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> tf.reshape(tf.<span class="bu">range</span>(<span class="dv">20</span>, dtype<span class="op">=</span>tf.float32), (<span class="dv">5</span>, <span class="dv">4</span>))</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> A  <span class="co"># 不能通过分配新内存将A克隆到B</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>A, A <span class="op">+</span> B</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co">&#39;&#39;&#39;</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="co">output:</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="co">(&lt;tf.Tensor: shape=(5, 4), dtype=float32, numpy=</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="co"> array([[ 0.,  1.,  2.,  3.],</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="co">        [ 4.,  5.,  6.,  7.],</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="co">        [ 8.,  9., 10., 11.],</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="co">        [12., 13., 14., 15.],</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="co">        [16., 17., 18., 19.]], dtype=float32)&gt;,</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="co"> &lt;tf.Tensor: shape=(5, 4), dtype=float32, numpy=</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a><span class="co"> array([[ 0.,  2.,  4.,  6.],</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a><span class="co">        [ 8., 10., 12., 14.],</span></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a><span class="co">        [16., 18., 20., 22.],</span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a><span class="co">        [24., 26., 28., 30.],</span></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a><span class="co">        [32., 34., 36., 38.]], dtype=float32)&gt;)</span></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a><span class="co">&#39;&#39;&#39;</span></span></code></pre></div>
<p>具体而言，两个矩阵按元素乘法成为<em>哈达玛积</em>（Hadamard product，数学符号 <span class="math inline">\(\odot\)</span>）。对于矩阵 <span class="math inline">\(\mathbf{B} \in \mathbb{R}^{m \times n}\)</span> ，其中第 <span class="math inline">\(i\)</span> 行和第 <span class="math inline">\(j\)</span> 列的元素是 <span class="math inline">\(b_{ij}\)</span> 。矩阵 <span class="math inline">\(\mathbf{A}\)</span> （定义 <code>2，1</code>）和 <span class="math inline">\(\mathbf{B}\)</span> 的哈达玛积为： <span class="math display">\[
\mathbf{A} \odot \mathbf{B} = \left[
\begin{matrix}
a_{11}b_{11} &amp; a_{12}b_{12} &amp; \dots &amp; a_{1n}b_{1n} \\
a_{21}b_{21} &amp; a_{22}b_{22} &amp; \dots &amp; a_{2n}b_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
a_{m1}b_{m1} &amp; a_{m2}b_{m2} &amp; \dots &amp; a_{mn}b_{mn} \\
\end{matrix}
\right], \tag{3, 1}
\]</span></p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>A <span class="op">*</span> B</span></code></pre></div>
<p>将张量乘以或加上一个标量不会改变张量的形状，其中张量的每个元素都将与标量相加或相乘。</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> tf.reshape(tf.<span class="bu">range</span>(<span class="dv">24</span>), (<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>))</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>a <span class="op">+</span> X, (a <span class="op">*</span> X).shape</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co">&#39;&#39;&#39;</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="co">output:</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="co">(&lt;tf.Tensor: shape=(2, 3, 4), dtype=int32, numpy=</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="co"> array([[[ 2,  3,  4,  5],</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="co">         [ 6,  7,  8,  9],</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="co">         [10, 11, 12, 13]],</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="co">        [[14, 15, 16, 17],</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="co">         [18, 19, 20, 21],</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a><span class="co">         [22, 23, 24, 25]]], dtype=int32)&gt;,</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a><span class="co"> TensorShape([2, 3, 4]))</span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a><span class="co">&#39;&#39;&#39;</span></span></code></pre></div>
<h3 id="降维">降维</h3>
<p>我们可以对任何张量计算其元素的和。在数学表示法中，我们使用 <span class="math inline">\(\sum\)</span> 来表示求和，为了表示向量中元素的总和，可以记为 <span class="math inline">\(\sum_{i=1}^{d}x_i\)</span> 。在 tensorflow 中，我们调用 <code>tf.reduce_sum()</code> 来计算求和。</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> tf.<span class="bu">range</span>(<span class="dv">4</span>, dtype<span class="op">=</span>tf.float32)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>x, tf.reduce_sum(x)</span></code></pre></div>
<p>我们也可以表示任何形状张量的元素和。例如，矩阵 <span class="math inline">\(\mathbf{A}\)</span> 中的元素和可以记为 <span class="math inline">\(\sum_{i=1}^{m}\sum_{j=1}^{n}a_{ij}\)</span> 。</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>A.shape, tf.reduce_sum(A)</span></code></pre></div>
<p>默认情况下，调用求和函数会沿所有的轴降低张量的维度，使它变为一个标量。 我们还可以指定张量沿哪一个轴来通过求和降低维度。</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>A_sum_axis0 <span class="op">=</span> tf.reduce_sum(A, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>A_sum_axis0, A_sum_axis0.shape</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="co">&#39;&#39;&#39;</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="co">output:</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="co">(&lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([40., 45., 50., 55.], dtype=float32)&gt;,</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="co"> TensorShape([4]))</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="co">&#39;&#39;&#39;</span></span></code></pre></div>
<p>其中 <code>axis</code> 的决定了求和函数将哪个轴合并，可以使用 <code>list</code> 进行多轴求和。显然，当 <code>axis=[0,1]</code> 时，对矩阵 <span class="math inline">\(\mathbf{A}\)</span> 的求和与直接求和无异。</p>
<h4 id="非降维求和">非降维求和</h4>
<p>有时候维度保持住还是非常有用的。</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>sum_A <span class="op">=</span> tf.reduce_sum(A, axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>sum_A</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="co">&#39;&#39;&#39;</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="co">output:</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;tf.Tensor: shape=(5, 1), dtype=float32, numpy=</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="co">array([[ 6.],</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="co">       [22.],</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="co">       [38.],</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="co">       [54.],</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="co">       [70.]], dtype=float32)&gt;</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="co">&#39;&#39;&#39;</span></span></code></pre></div>
<p>如果我们想沿某个轴计算<code>A</code>元素的累积总和，比如<code>axis=0</code>（按行计算），我们可以调用<code>tf.cumsum()</code>函数。此函数不会沿任何轴降低输入张量的维度。</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>tf.cumsum(A, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="co">&#39;&#39;&#39;</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="co">output:</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;tf.Tensor: shape=(5, 4), dtype=float32, numpy=</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="co">array([[ 0.,  1.,  2.,  3.],</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="co">       [ 4.,  6.,  8., 10.],</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="co">       [12., 15., 18., 21.],</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="co">       [24., 28., 32., 36.],</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="co">       [40., 45., 50., 55.]], dtype=float32)&gt;</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a><span class="co">&#39;&#39;&#39;</span></span></code></pre></div>
<h3 id="点积">点积</h3>
<p>所谓点积（Dot Product），即给定两个向量 <span class="math inline">\(\mathbf{x}, \mathbf{y} \in \mathbb{R}^d\)</span> ，它们的点积为 <span class="math inline">\(\mathbf{x} \cdot \mathbf{y}\)</span> （或 <span class="math inline">\(\left&lt;\mathbf{x}, \mathbf{y}\right&gt;\)</span>），计算为相同位置的按元素成绩的和： <span class="math inline">\(\mathbf{x \cdot y} = \sum_{i=1}^{d}x_iy_i\)</span> 。</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> tf.ones(<span class="dv">4</span>, dtype<span class="op">=</span>tf.float32)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>x, y, tf.tensordot(x, y, axes<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="co">&#39;&#39;&#39;</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="co">output:</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="co">(&lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 1., 2., 3.], dtype=float32)&gt;,</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="co"> &lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([1., 1., 1., 1.], dtype=float32)&gt;,</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="co"> &lt;tf.Tensor: shape=(), dtype=float32, numpy=6.0&gt;)</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="co">&#39;&#39;&#39;</span></span></code></pre></div>
<p>其实也可以直接相加。</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>tf.redice_sum(x <span class="op">*</span> y)</span></code></pre></div>
<p>从代数上看， <span class="math inline">\(\mathbf{x} \cdot \mathbf{y}\)</span> 实际上等于 <span class="math inline">\(\mathbf{x}^{\mathsf{T}} \mathbf{y}\)</span> ；从几何上看，点积可以直观的定义为 <span class="math inline">\(\mathbf{a \cdot b} = \mathbf{|a||b|}\cos \theta\)</span> ， <span class="math inline">\(\theta\)</span> 为 <span class="math inline">\(\mathbf{A}\)</span> 和 <span class="math inline">\(\mathbf{B}\)</span> 的夹角，因此，不难理解，两个向量的点积可以理解为向量 <span class="math inline">\(\mathbf{A}\)</span> 在向量 <span class="math inline">\(\mathbf{B}\)</span> 上的投影再乘以 <span class="math inline">\(\mathbf{B}\)</span> 的长度。</p>
<h3 id="矩阵向量积">矩阵向量积</h3>
<p>现在，我们知道如何计算点积，我们可以开始计算<em>矩阵向量积</em>（matrix-vector product）了。这个实际上是一个过渡内容。</p>
<p>我们首先将矩阵 <span class="math inline">\(\mathbf{A}\)</span> 用行向量表示： <span class="math display">\[
\mathbf{A} = \left[
\begin{matrix}
\mathbf{a_1^{\mathsf{T}}} \\
\mathbf{a_2^{\mathsf{T}}} \\
\vdots \\
\mathbf{a_m^{\mathsf{T}}} \\
\end{matrix}
\right] \tag{4, 1}
\]</span> 其中，<span class="math inline">\(\mathbf{a_i^{\mathsf{T}}} \in \mathbb{R}^n\)</span> 都是行向量，表示矩阵第 <span class="math inline">\(i\)</span> 行。矩阵向量积 <span class="math inline">\(\mathbf{Ax}\)</span> 是一个长度为 <span class="math inline">\(m\)</span> 的列向量： <span class="math display">\[
\mathbf{Ax} = \left[
\begin{matrix}
\mathbf{a_1^{\mathsf{T}}} \\
\mathbf{a_2^{\mathsf{T}}} \\
\vdots \\
\mathbf{a_m^{\mathsf{T}}} \\
\end{matrix}
\right]\mathbf{x}= \left[
\begin{matrix}
\mathbf{a_1^{\mathsf{T}}x} \\
\mathbf{a_2^{\mathsf{T}}x} \\
\vdots \\
\mathbf{a_m^{\mathsf{T}}x} \\
\end{matrix}
\right] \tag{4, 2}
\]</span></p>
<h3 id="矩阵乘法">矩阵乘法</h3>
<p>现在我们学习矩阵乘法。</p>
<p>假设我们有两个矩阵 <span class="math inline">\(\mathbf{A} \in \mathbb{R}^{n\times k}\)</span> 和 <span class="math inline">\(\mathbf{B} \in \mathbb{R}^{n\times k}\)</span> ： <span class="math display">\[
\mathbf{A} = \left[
\begin{matrix}
a_{11} &amp; a_{12} &amp; \dots &amp; a_{1k} \\
a_{21} &amp; a_{22} &amp; \dots &amp; a_{2k} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
a_{n1} &amp; a_{n2} &amp; \dots &amp; a_{nk} \\
\end{matrix}
\right],
\mathbf{B} = \left[
\begin{matrix}
b_{11} &amp; b_{12} &amp; \dots &amp; b_{1k} \\
b_{21} &amp; b_{22} &amp; \dots &amp; b_{2k} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
b_{n1} &amp; b_{n2} &amp; \dots &amp; b_{nk} \\
\end{matrix}
\right] \tag{5, 1}
\]</span> 用行向量 <span class="math inline">\(\mathbf{a_i^{\mathsf{T}}} \in \mathbb{R}^k\)</span> 表示矩阵 <span class="math inline">\(\mathbf{A}\)</span> 的第 <span class="math inline">\(i\)</span> 行，并让列向量 <span class="math inline">\(\mathbf{b_j} \in \mathbb{R}^k\)</span> 表示矩阵 <span class="math inline">\(\mathbf{B}\)</span> 的第 <span class="math inline">\(j\)</span> 列，则 <span class="math display">\[
\mathbf{A} = \left[
\begin{matrix}
\mathbf{a_1^{\mathsf{T}}} \\
\mathbf{a_2^{\mathsf{T}}} \\
\vdots \\
\mathbf{a_n^{\mathsf{T}}} \\
\end{matrix}
\right],
\mathbf{B} = \left[
\begin{matrix}
\mathbf{b_1} &amp;,
\mathbf{b_2} &amp;
\cdots &amp;
\mathbf{b_m}
\end{matrix}
\right] \tag{5, 2}
\]</span> 当我们简单地将每个元素 <span class="math inline">\(c_{ij}\)</span> 计算为点积 <span class="math inline">\(\mathbf{a_i^\mathsf{T}b_j}\)</span>: <span class="math display">\[
\mathbf{C} =
\mathbf{AB} = \left[
\begin{matrix}
\mathbf{a_1^{\mathsf{T}}} \\
\mathbf{a_2^{\mathsf{T}}} \\
\vdots \\
\mathbf{a_m^{\mathsf{T}}} \\
\end{matrix}
\right]\left[
\begin{matrix}
\mathbf{b_1} &amp;,
\mathbf{b_2} &amp;
\cdots &amp;
\mathbf{b_m}
\end{matrix}
\right] = \left[
\begin{matrix}
\mathbf{a_1^{\mathsf{T}}b_1} &amp; \mathbf{a_1^{\mathsf{T}}b_2} &amp; \cdots &amp; \mathbf{a_1^{\mathsf{T}}b_m} \\
\mathbf{a_2^{\mathsf{T}}b_1} &amp; \mathbf{a_2^{\mathsf{T}}b_2} &amp; \cdots &amp; \mathbf{a_2^{\mathsf{T}}b_m} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\mathbf{a_n^{\mathsf{T}}b_1} &amp; \mathbf{a_n^{\mathsf{T}}b_2} &amp; \cdots &amp; \mathbf{a_n^{\mathsf{T}}b_m} \\
\end{matrix}
\right] \tag{5, 3}
\]</span> 我们可以将矩阵-矩阵乘法 <span class="math inline">\(\mathbf{AB}\)</span> 看作是简单地执行 <span class="math inline">\(m\)</span> 次矩阵-向量积，并将结果拼接在一起，形成一个 <span class="math inline">\(m\times n\)</span> 矩阵。在下面的代码中，我们在 <code>A</code> 和 <code>B</code> 上执行矩阵乘法。这里的 <code>A</code> 是一个5行4列的矩阵， <code>B</code> 是一个4行3列的矩阵。相乘后，我们得到了一个5行3列的矩阵。</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> tf.ones((<span class="dv">4</span>, <span class="dv">3</span>), dtype<span class="op">=</span>float64)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>tf.matmul(A, B)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="co">&#39;&#39;&#39;</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="co">output:</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;tf.Tensor: shape=(5, 3), dtype=float32, numpy=</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a><span class="co">array([[ 6.,  6.,  6.],</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="co">       [22., 22., 22.],</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a><span class="co">       [38., 38., 38.],</span></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a><span class="co">       [54., 54., 54.],</span></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a><span class="co">       [70., 70., 70.]], dtype=float32)&gt;</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a><span class="co">&#39;&#39;&#39;</span></span></code></pre></div>
<h3 id="范数">范数</h3>



    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">文章作者</span>
    <span class="item-content">bigshans</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">上次更新</span>
    <span class="item-content">
        2021-10-18
        
    </span>
  </p>
  
  
</div>
<footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/machine-learing/">machine learing</a>
          <a href="/tags/tensorflow/">tensorflow</a>
          </div>
      <nav class="post-nav">
        
        <a class="next" href="/post/collabora-install/">
            <span class="next-text nav-default">安装 Collabora Office</span>
            <span class="next-text nav-mobile">下一篇</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="https://github.com/bigshans" class="iconfont icon-github" title="github"></a>
  <a href="https://bigshans.github.io/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://gohugo.io">Hugo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2020 - 
    2021<span class="heart"><i class="iconfont icon-heart"></i></span><span>bigshans</span>
  </span>
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  <script src="/lib/highlight/highlight.pack.js?v=20171001"></script>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>



<script type="text/javascript" src="/js/main.min.c99b103c33d1539acf3025e1913697534542c4a5aa5af0ccc20475ed2863603b.js"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        }
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js" integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin="anonymous"></script>








</body>
</html>
